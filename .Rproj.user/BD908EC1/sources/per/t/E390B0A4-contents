---
title: "XGBoost"
author: "Nayan Bakhadyo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r XGBoost libraries and files, include = FALSE, echo=FALSE}
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
```
Removed Storey column. 
XGBoost Data Prep
```{r Data prep, echo=FALSE}
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols

# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))

# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
population_data_categorical_encoded <- as.data.frame(predict(dummies, newdata = population_data[, categorical_cols]))

# Combine the scaled numeric data with the one-hot encoded categorical data
population_data_scaled <- cbind(population_data_numeric_scaled, population_data_categorical_encoded)

# Add the target variable (total_pop) back to the dataset
population_data_scaled$total_pop <- population_data$total_pop


```

XGBoost Data Split
```{r XGBoost Data Split, echo=FALSE}
# Set seed for reproducibility
set.seed(123)

# Split data into training (80%) and testing (20%)
train_indices <- createDataPartition(population_data_scaled$total_pop, p = 0.8, list = FALSE)
train_data <- population_data_scaled[train_indices, ]
test_data <- population_data_scaled[-train_indices, ]


```


XGBoost Training
```{r XGBoost Training, echo=FALSE}
# Convert training data to a matrix (XGBoost requires matrix format)
train_matrix <- as.matrix(train_data[, -ncol(train_data)])  # Exclude the target variable
train_label <- train_data$total_pop

# Convert testing data to a matrix
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$total_pop

# Create DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)

# Define parameters for XGBoost
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror",  # For regression
  eta = 0.1,                       # Learning rate
  max_depth = 6,                   # Maximum depth of trees
  subsample = 0.8,                 # Subsampling ratio of training data
  colsample_bytree = 0.8           # Subsampling ratio of columns
)

# Train the XGBoost model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), early_stopping_rounds = 10, verbose = 0)

# Predict on test data
pred_xgb <- predict(xgb_model, newdata = dtest)

pred_xgb <- round(pred_xgb)

# Create a dataframe for plotting
results <- data.frame(Actual = test_data$total_pop, Predicted = pred_xgb)

# Plot Actual vs Predicted
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(title = "Actual vs Predicted", x = "Actual Total Population", y = "Predicted Total Population") +
  theme_minimal()
```

XGBoost Performance
```{r XGBoost Performance, echo=FALSE}
# Calculate Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb - test_label)^2)

# Calculate R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb - test_label)^2)          # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)

# Print metrics
cat("XGBoost MSE:", mse_xgb, "\n")
cat("XGBoost R-squared:", r_squared_xgb, "\n")

```

XGBoost on test data
```{r XGBoost on Test Data, echo=FALSE}
# Predict on the test dataset
pred_xgb_test <- predict(xgb_model, newdata = dtest)

# Compute metrics for regression evaluation

# Mean Absolute Error (MAE)
mae_xgb <- mean(abs(pred_xgb_test - test_label))

# Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb_test - test_label)^2)

#MAPE
mape <- mean(abs((pred_xgb_test - test_label) / test_label)) * 100  # MAPE in percentage

# Compute accuracy as a percentage
accuracy <- 100 - mape  # Accuracy percentage

# Root Mean Squared Error (RMSE)
rmse_xgb <- sqrt(mse_xgb)

# R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb_test - test_label)^2)     # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)

# Print metrics
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
```

```{r Save XGBoost, echo=FALSE}
# Save the model to a binary file
xgb.save(xgb_model, "Population_xgb_model2.bin")
cat("Model saved as Population_xgb_model2.bin\n")
```
