cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-storey)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
population_data_categorical_encoded <- as.data.frame(predict(dummies, newdata = population_data[, categorical_cols]))
# Combine the scaled numeric data with the one-hot encoded categorical data
population_data_scaled <- cbind(population_data_numeric_scaled, population_data_categorical_encoded)
# Add the target variable (total_pop) back to the dataset
population_data_scaled$total_pop <- population_data$total_pop
# Set seed for reproducibility
set.seed(123)
# Split data into training (80%) and testing (20%)
train_indices <- createDataPartition(population_data_scaled$total_pop, p = 0.8, list = FALSE)
train_data <- population_data_scaled[train_indices, ]
test_data <- population_data_scaled[-train_indices, ]
# Convert training data to a matrix (XGBoost requires matrix format)
train_matrix <- as.matrix(train_data[, -ncol(train_data)])  # Exclude the target variable
train_label <- train_data$total_pop
# Convert testing data to a matrix
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$total_pop
# Create DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)
# Define parameters for XGBoost
params <- list(
booster = "gbtree",
objective = "reg:squarederror",  # For regression
eta = 0.1,                       # Learning rate
max_depth = 6,                   # Maximum depth of trees
subsample = 0.8,                 # Subsampling ratio of training data
colsample_bytree = 0.8           # Subsampling ratio of columns
)
# Train the XGBoost model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), early_stopping_rounds = 10, verbose = 0)
# Predict on test data
pred_xgb <- predict(xgb_model, newdata = dtest)
pred_xgb <- round(pred_xgb)
# Create a dataframe for plotting
results <- data.frame(Actual = test_data$total_pop, Predicted = pred_xgb_test)
# Plot Actual vs Predicted
ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs Predicted", x = "Actual Total Population", y = "Predicted Total Population") +
theme_minimal()
# Calculate Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb - test_label)^2)
# Calculate R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb - test_label)^2)          # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("XGBoost MSE:", mse_xgb, "\n")
cat("XGBoost R-squared:", r_squared_xgb, "\n")
# Predict on the test dataset
pred_xgb_test <- predict(xgb_model, newdata = dtest)
# Compute metrics for regression evaluation
# Mean Absolute Error (MAE)
mae_xgb <- mean(abs(pred_xgb_test - test_label))
# Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb_test - test_label)^2)
#MAPE
mape <- mean(abs((pred_xgb_test - test_label) / test_label)) * 100  # MAPE in percentage
# Compute accuracy as a percentage
accuracy <- 100 - mape  # Accuracy percentage
# Root Mean Squared Error (RMSE)
rmse_xgb <- sqrt(mse_xgb)
# R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb_test - test_label)^2)     # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
# Save the model to a binary file
xgb.save(xgb_model, "Population_xgb_model2.bin")
cat("Model saved as Population_xgb_model2.bin\n")
View(population_data)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
population_data <- population_data %>% select(-Category)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
population_data <- population_data %>% select(-Category)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
population_data <- population_data %>% select(-Category)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
population_data_categorical_encoded <- as.data.frame(predict(dummies, newdata = population_data[, categorical_cols]))
# Combine the scaled numeric data with the one-hot encoded categorical data
population_data_scaled <- cbind(population_data_numeric_scaled, population_data_categorical_encoded)
# Add the target variable (total_pop) back to the dataset
population_data_scaled$total_pop <- population_data$total_pop
# Set seed for reproducibility
set.seed(123)
# Split data into training (80%) and testing (20%)
train_indices <- createDataPartition(population_data_scaled$total_pop, p = 0.8, list = FALSE)
train_data <- population_data_scaled[train_indices, ]
test_data <- population_data_scaled[-train_indices, ]
# Convert training data to a matrix (XGBoost requires matrix format)
train_matrix <- as.matrix(train_data[, -ncol(train_data)])  # Exclude the target variable
train_label <- train_data$total_pop
# Convert testing data to a matrix
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$total_pop
# Create DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)
# Define parameters for XGBoost
params <- list(
booster = "gbtree",
objective = "reg:squarederror",  # For regression
eta = 0.1,                       # Learning rate
max_depth = 6,                   # Maximum depth of trees
subsample = 0.8,                 # Subsampling ratio of training data
colsample_bytree = 0.8           # Subsampling ratio of columns
)
# Train the XGBoost model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), early_stopping_rounds = 10, verbose = 0)
# Predict on test data
pred_xgb <- predict(xgb_model, newdata = dtest)
pred_xgb <- round(pred_xgb)
# Create a dataframe for plotting
results <- data.frame(Actual = test_data$total_pop, Predicted = pred_xgb_test)
# Plot Actual vs Predicted
ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs Predicted", x = "Actual Total Population", y = "Predicted Total Population") +
theme_minimal()
# Calculate Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb - test_label)^2)
# Calculate R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb - test_label)^2)          # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("XGBoost MSE:", mse_xgb, "\n")
cat("XGBoost R-squared:", r_squared_xgb, "\n")
# Predict on the test dataset
pred_xgb_test <- predict(xgb_model, newdata = dtest)
# Compute metrics for regression evaluation
# Mean Absolute Error (MAE)
mae_xgb <- mean(abs(pred_xgb_test - test_label))
# Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb_test - test_label)^2)
#MAPE
mape <- mean(abs((pred_xgb_test - test_label) / test_label)) * 100  # MAPE in percentage
# Compute accuracy as a percentage
accuracy <- 100 - mape  # Accuracy percentage
# Root Mean Squared Error (RMSE)
rmse_xgb <- sqrt(mse_xgb)
# R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb_test - test_label)^2)     # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
# Save the model to a binary file
xgb.save(xgb_model, "Population_xgb_model2.bin")
cat("Model saved as Population_xgb_model2.bin\n")
# Convert training data to a matrix (XGBoost requires matrix format)
train_matrix <- as.matrix(train_data[, -ncol(train_data)])  # Exclude the target variable
train_label <- train_data$total_pop
# Convert testing data to a matrix
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$total_pop
# Create DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)
# Define parameters for XGBoost
params <- list(
booster = "gbtree",
objective = "reg:squarederror",  # For regression
eta = 0.1,                       # Learning rate
max_depth = 6,                   # Maximum depth of trees
subsample = 0.8,                 # Subsampling ratio of training data
colsample_bytree = 0.8           # Subsampling ratio of columns
)
# Train the XGBoost model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), early_stopping_rounds = 10, verbose = 0)
# Predict on test data
pred_xgb <- predict(xgb_model, newdata = dtest)
pred_xgb <- round(pred_xgb)
# Create a dataframe for plotting
results <- data.frame(Actual = test_data$total_pop, Predicted = pred_xgb)
# Plot Actual vs Predicted
ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs Predicted", x = "Actual Total Population", y = "Predicted Total Population") +
theme_minimal()
# Calculate Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb - test_label)^2)
# Calculate R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb - test_label)^2)          # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("XGBoost MSE:", mse_xgb, "\n")
cat("XGBoost R-squared:", r_squared_xgb, "\n")
# Predict on the test dataset
pred_xgb_test <- predict(xgb_model, newdata = dtest)
# Compute metrics for regression evaluation
# Mean Absolute Error (MAE)
mae_xgb <- mean(abs(pred_xgb_test - test_label))
# Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb_test - test_label)^2)
#MAPE
mape <- mean(abs((pred_xgb_test - test_label) / test_label)) * 100  # MAPE in percentage
# Compute accuracy as a percentage
accuracy <- 100 - mape  # Accuracy percentage
# Root Mean Squared Error (RMSE)
rmse_xgb <- sqrt(mse_xgb)
# R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb_test - test_label)^2)     # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
library(tidyverse)
library(xgboost)
library(caret)
library(fastDummies)
library(ggplot2)
library(dplyr)
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
population_data_categorical_encoded <- as.data.frame(predict(dummies, newdata = population_data[, categorical_cols]))
# Combine the scaled numeric data with the one-hot encoded categorical data
population_data_scaled <- cbind(population_data_numeric_scaled, population_data_categorical_encoded)
# Add the target variable (total_pop) back to the dataset
population_data_scaled$total_pop <- population_data$total_pop
# Set seed for reproducibility
set.seed(123)
# Split data into training (80%) and testing (20%)
train_indices <- createDataPartition(population_data_scaled$total_pop, p = 0.8, list = FALSE)
train_data <- population_data_scaled[train_indices, ]
test_data <- population_data_scaled[-train_indices, ]
# Convert training data to a matrix (XGBoost requires matrix format)
train_matrix <- as.matrix(train_data[, -ncol(train_data)])  # Exclude the target variable
train_label <- train_data$total_pop
# Convert testing data to a matrix
test_matrix <- as.matrix(test_data[, -ncol(test_data)])
test_label <- test_data$total_pop
# Create DMatrix objects for XGBoost
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
dtest <- xgb.DMatrix(data = test_matrix, label = test_label)
# Define parameters for XGBoost
params <- list(
booster = "gbtree",
objective = "reg:squarederror",  # For regression
eta = 0.1,                       # Learning rate
max_depth = 6,                   # Maximum depth of trees
subsample = 0.8,                 # Subsampling ratio of training data
colsample_bytree = 0.8           # Subsampling ratio of columns
)
# Train the XGBoost model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), early_stopping_rounds = 10, verbose = 0)
# Predict on test data
pred_xgb <- predict(xgb_model, newdata = dtest)
pred_xgb <- round(pred_xgb)
# Create a dataframe for plotting
results <- data.frame(Actual = test_data$total_pop, Predicted = pred_xgb)
# Plot Actual vs Predicted
ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, color = "red") +
labs(title = "Actual vs Predicted", x = "Actual Total Population", y = "Predicted Total Population") +
theme_minimal()
# Calculate Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb - test_label)^2)
# Calculate R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb - test_label)^2)          # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("XGBoost MSE:", mse_xgb, "\n")
cat("XGBoost R-squared:", r_squared_xgb, "\n")
# Predict on the test dataset
pred_xgb_test <- predict(xgb_model, newdata = dtest)
# Compute metrics for regression evaluation
# Mean Absolute Error (MAE)
mae_xgb <- mean(abs(pred_xgb_test - test_label))
# Mean Squared Error (MSE)
mse_xgb <- mean((pred_xgb_test - test_label)^2)
#MAPE
mape <- mean(abs((pred_xgb_test - test_label) / test_label)) * 100  # MAPE in percentage
# Compute accuracy as a percentage
accuracy <- 100 - mape  # Accuracy percentage
# Root Mean Squared Error (RMSE)
rmse_xgb <- sqrt(mse_xgb)
# R-squared
sst <- sum((test_label - mean(test_label))^2)  # Total sum of squares
sse <- sum((pred_xgb_test - test_label)^2)     # Sum of squared errors
r_squared_xgb <- 1 - (sse / sst)
# Print metrics
cat("Mean Absolute Error (MAE):", mae_xgb, "\n")
cat("Mean Squared Error (MSE):", mse_xgb, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_xgb, "\n")
cat("R-squared:", r_squared_xgb, "\n")
cat("MAPE:", mape, "\n")
cat("Accuracy:", accuracy, "%\n")
# Save the model to a binary file
xgb.save(xgb_model, "Population_xgb_model2.bin")
cat("Model saved as Population_xgb_model2.bin\n")
library(xgboost)
library(data.table) # For fread function
population_data <- fread('final_population_dataset.csv') # Use fread for fast reading
population_data$height <- NULL # Remove height column if necessary
population_data <- read_csv('final_population_dataset.csv')
# Remove the height column using dplyr
population_data <- population_data %>% select(-Storey)
# Separate numeric and categorical features
numeric_cols <- sapply(population_data, is.numeric)
categorical_cols <- !numeric_cols
# Scale (normalize) numeric features
population_data_numeric_scaled <- as.data.frame(scale(population_data[, numeric_cols]))
# One-hot encode categorical features
dummies <- dummyVars("~ .", data = population_data[, categorical_cols], fullRank = TRUE)
population_data_categorical_encoded <- as.data.frame(predict(dummies, newdata = population_data[, categorical_cols]))
# Combine the scaled numeric data with the one-hot encoded categorical data
population_data_scaled <- cbind(population_data_numeric_scaled, population_data_categorical_encoded)
# Add the target variable (total_pop) back to the dataset
population_data_scaled$total_pop <- population_data$total_pop
train_data <- population_data %>% select(-total_pop)
# Convert to matrix
train_matrix <- as.matrix(train_data[, -1])  # Exclude the target variable
train_label <- train_data$total_pop
# Create DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area.sqm, Water, Bareland, Vegetation, Built.up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built.up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, school_acc)
# Assuming total_pop is the target variable
train_data <- population_data[total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, school_acc]
library(xgboost)
library(data.table) # For fread function
population_data <- fread('final_population_dataset.csv') # Use fread for fast reading
population_data$Storey <- NULL # Remove height column if necessary
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area.sqm, Water, Bareland, Vegetation, Built.up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
library(xgboost)
library(data.table) # For fread function
population_data <- fread('final_population_dataset.csv') # Use fread for fast reading
population_data$Storey <- NULL # Remove height column if necessary
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built-up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
library(xgboost)
library(data.table) # For fread function
population_data <- fread('final_population_dataset.csv') # Use fread for fast reading
population_data$Storey <- NULL # Remove height column if necessary
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, Storey, school_acc)]
# Convert to matrix
train_matrix <- as.matrix(train_data[, -1])  # Exclude the target variable
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor,Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor,Storey, school_acc)]
# Assuming total_pop is the target variable
train_data <- population_data[, .(total_pop, Area_sqm, Water, Bareland, Vegetation, Built_up, public_roa, bank_acces, Road, atm_access, hospital_a, road_densi, Settlement, Department, super_stor, school_acc)]
# Convert to matrix
train_matrix <- as.matrix(train_data[, -1])  # Exclude the target variable
train_label <- train_data$total_pop
# Create DMatrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_label)
params <- list(
booster = "gbtree",
objective = "reg:squarederror",  # For regression
eta = 0.17805470445866273,        # Learning rate
max_depth = 8,                    # Maximum depth of trees
gamma = 0.01278105621936969,      # Minimum loss reduction required to make a further partition
min_child_weight = 6,             # Minimum sum of instance weight (hessian) needed in a child
subsample = 0.9359079020242138,   # Subsampling ratio of training data
colsample_bytree = 0.6647772113228527 # Subsampling ratio of columns
)
# Perform cross-validation
cv_results <- xgb.cv(
params = params,
data = dtrain,
nrounds = 100,                     # Number of boosting rounds
nfold = 5,                         # Number of folds for cross-validation
metrics = "rmse",                  # Metric to evaluate
early_stopping_rounds = 10,        # Stop if no improvement in 10 rounds
verbose = 1                        # Print progress
)
# View CV results
print(cv_results)
# Get the best RMSE
best_rmse <- min(cv_results$evaluation_log$test_rmse_mean)
cat("Best RMSE from CV:", best_rmse, "\n")
